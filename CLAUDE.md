# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

TAP (Testing Assistant Project) is a Bun-based CLI tool that uses AI-powered test generation and human-in-the-loop workflow to create and execute ephemeral testing scenarios from GitHub PRs and Jira tickets. It combines Claude API for intelligent test generation, Claude Code for human refinement, and CUA (Computer Use Agent) for automated test execution with Docker-based containerized testing.

## Platform Support

**Currently Supported:**
- ✅ **Linux** (Operating System) via Docker-based CUA execution environment
  - ✅ Desktop applications
  - ✅ Browser Extensions (Chrome, Firefox, etc.)
  - ✅ Web Applications (accessed via browser)
  - ✅ CLI tools (command-line applications)

**Not Yet Supported (Future Releases):**
- ❌ macOS (Operating System)
- ❌ Windows (Operating System)
- ❌ iOS (Operating System)
- ❌ Android (Operating System)
- ❌ Mobile Apps (require iOS/Android)

All test scenarios generated by TAP will target the **Linux** platform only. The AI generator is configured to reject scenarios for unsupported operating systems and will automatically force all tests to use "platform": "Linux". The client type (Desktop, Browser Extension, Web App, CLI) will be selected based on what the PR is testing.

## Development Commands

### Primary Tasks

```bash
# Human-in-the-loop workflow (recommended)
bun run start generate-tests <pr-url>              # Generate AI scenarios + export context
bun run start execute-scenarios --file <refined>   # Execute refined scenarios

# Direct execution (combines generation + execution)
# Note: Use generate-tests + execute-scenarios for recommended human-in-the-loop workflow

# Development and setup
bun run dev                                        # Development with file watching
bun run start setup                               # Setup and configuration (interactive, required)

# Setup options for test execution
bun run start generate-tests <pr-url> --setup     # Add PR-specific setup instructions
# For direct execution, chain the commands:
# bun run start generate-tests <pr-url> && bun run start execute-scenarios --file ./test-pr-*/generated-scenarios.json
bun run start execute-scenarios --file <refined> --instructions  # Add PR-specific setup instructions
```

### Build Commands

```bash
# Build main TAP executable
bun run build

# Build for multiple platforms
bun run build:cross

# Platform-specific builds
bun run build:linux
bun run build:windows
bun run build:macos

# Clean build artifacts
bun run clean

# Code formatting and linting
bun run format
bun run lint
```

## Code Architecture

### Core Structure

- `src/main.ts` - CLI entry point using Commander.js framework
- `src/commands/` - Command implementations (generate-tests, execute-scenarios, setup)
- `src/services/` - Business logic services

### Key Services

- `GitHubService` - PR analysis and diff processing
- `AtlassianService` - Jira ticket and Confluence page integration
- `ContextGatheringService` - Comprehensive context collection from multiple sources
- `OnyxContextService` - Enhanced product context from Onyx AI (optional)
- `TestExecutionService` - Test execution coordination and management
- `ConfigService` - Configuration management for API credentials

### Data Flow (Human-in-the-Loop)

1. **Context Gathering** → GitHub PR analysis + Jira tickets + Confluence docs
2. **AI Generation** → Claude API creates intelligent scenarios from full context
3. **Context Export** → Comprehensive data export for human review
4. **Human Refinement** → Claude Code assists with scenario review and improvement
5. **Execution** → CUA (Computer Use Agent) runs refined scenarios with Docker-based computer control
6. **QA Reporting** → Structured output with AI insights and execution artifacts

## Configuration

TAP requires configuration before use:

### 1. Interactive Setup (Required)

```bash
bun run start setup
```

Creates `~/.tap/config.json` with your API credentials and **required app setup instructions**.

### 2. Environment Variables (Alternative)

- `GITHUB_TOKEN` - GitHub Personal Access Token
- `ATLASSIAN_API_TOKEN` - Unified token for Jira and Confluence
- `ATLASSIAN_EMAIL` - Atlassian account email
- `ATLASSIAN_BASE_URL` - Atlassian instance URL (e.g., https://company.atlassian.net)
- `TAP_APP_SETUP_INSTRUCTIONS` - Natural language app setup instructions (required for test execution)
- `ONYX_BASE_URL` - Onyx instance URL (optional - defaults to https://api.onyx.app)
- `ONYX_API_KEY` - Onyx AI API key (optional - for enhanced product context)

### 3. Claude CLI for AI Generation (Required)

TAP requires Claude CLI for AI-powered test scenario generation. Install and authenticate:

```bash
# Install Claude CLI for intelligent test scenario generation
npm install -g @anthropic-ai/claude-cli

# Authenticate with your Anthropic account
claude auth

# Verify installation
claude --version
```

### 4. CUA (Computer Use Agent) for Test Execution (Required)

TAP requires CUA with Docker for automated test execution with containerized computer control.

#### Prerequisites

**Required:**

- **Python 3.10+**: For running CUA agent
- **Docker**: Required for containerized test execution
  - macOS: [Docker Desktop](https://www.docker.com/products/docker-desktop)
  - Linux: `sudo apt install docker.io` (or equivalent)
  - Windows: Docker Desktop with WSL2 backend

#### Installation

```bash
bun run start setup
```

TAP will automatically detect if CUA is missing and install it during setup.

**Prerequisites:**

- Python 3.10+ must be installed first
- Docker must be installed and running

The system automatically validates CUA setup and Docker availability before running test execution commands.

## App Setup Instructions

TAP requires natural language instructions describing how to access and authenticate with your application for testing. These instructions are provided to CUA (Computer Use Agent) as part of the execution context.

### Setup Layers

TAP supports multiple layers of setup instructions to handle different testing scenarios:

#### 1. Base App Setup (Required - configured in `tap setup`)

```
Example:
1. Navigate to https://staging.myapp.com
2. If logged out, use test account: testuser@company.com / TestPass123
3. Click 'Admin Panel' in top menu to access admin features
```

#### 2. PR-Specific Setup (Optional - use `--setup` flag with generate-tests)

```
Example:
• This PR requires running: npm run build
• New feature flag: FEATURE_X=true
• Test with port 3001 instead of default 3000
• Download build artifact from GitHub Actions run #123
```

#### 3. Session-Specific Setup (Interactive - prompted during execute-scenarios)

```
Example:
• Start local development server: npm run dev
• Clear browser cache and logout first
• Use specific test data: import testdata.sql
```

### Setup Best Practices

- **Be specific**: Include exact URLs, credentials, and steps
- **Include authentication**: Provide test accounts and passwords
- **Environment details**: Specify ports, feature flags, build requirements
- **Prerequisites**: Mention any setup steps like starting servers or importing data
- **Natural language**: Write instructions as you would tell a human tester

## GitHub Actions Artifact Download

TAP includes built-in tools for securely downloading GitHub Actions artifacts during test execution. This is useful when your test scenarios need to download and test build artifacts, compiled binaries, or packages from GitHub Actions workflow runs.

### Available Tools

The following custom tools are automatically available to the CUA agent during test execution:

#### `list_github_artifacts(workflow_run_id)`

Lists all artifacts from a GitHub Actions workflow run.

**Parameters:**

- `workflow_run_id` (string): Workflow run ID (numeric) or full GitHub Actions URL
  - Examples: `"1234567890"` or `"https://github.com/owner/repo/actions/runs/1234567890"`

**Returns:**

- JSON string with artifact list including IDs, names, sizes, and metadata
- Error message if token missing, workflow not found, or API error

**Example response:**

```json
{
  "success": true,
  "workflow_run_id": "1234567890",
  "repository": "owner/repo",
  "count": 2,
  "artifacts": [
    {
      "id": 123456789,
      "name": "build-output",
      "size_bytes": 1024000,
      "expired": false,
      "created_at": "2025-01-15T10:30:00Z"
    }
  ]
}
```

#### `download_github_artifact(artifact_id, destination_filename)`

Downloads a specific GitHub Actions artifact by ID to the Docker container.

**Parameters:**

- `artifact_id` (string): Artifact ID from `list_github_artifacts`
- `destination_filename` (string, optional): Custom filename (default: `artifact-{id}.zip`)

**Returns:**

- JSON string with download status, file path, and size
- Error message if token missing, artifact not found, or download fails

**Download location:** `/home/kasm-user/Desktop/` in the Docker container

**Example response:**

```json
{
  "success": true,
  "artifact_id": "123456789",
  "repository": "owner/repo",
  "file_path": "/home/kasm-user/Desktop/build-output.zip",
  "size_bytes": 1024000,
  "message": "Artifact downloaded successfully to /home/kasm-user/Desktop/build-output.zip"
}
```

### Usage Example

When generating test scenarios with PR-specific setup instructions, you can include artifact download steps:

```bash
# Generate tests with PR-specific setup
bun run start generate-tests <pr-url> --setup

# In the PR-specific setup prompt, include instructions like:
• Download the build artifact from workflow run https://github.com/owner/repo/actions/runs/1234567890
• Use list_github_artifacts to find the correct artifact
• Download the artifact using download_github_artifact
• Extract the artifact and test the built application
```

The AI agent will automatically use these tools during test execution to download the necessary artifacts.

### Security

- **Token Storage**: GitHub token is stored securely in `~/.tap/config.json` (configured via `tap setup`)
- **Token Passing**: Token passed to CUA via environment variables only
- **No Prompt Leakage**: Token never appears in AI prompts or execution logs
- **Repository Context**: Owner and repo information auto-detected from PR analysis
- **Required Permissions**: GitHub token must have `repo` scope to access artifacts

### Error Handling

Both tools return clear error messages when issues occur:

- **`"GITHUB_TOKEN not available in environment"`** - Run `tap setup` to configure GitHub token
- **`"Repository context not available"`** - Ensure executing with proper PR analysis context
- **`"Workflow run not found"`** - Verify the workflow run ID exists in the repository
- **`"GitHub API authentication failed: HTTP 401/403"`** - Check token has `repo` scope permissions
- **`"Artifact not found"`** - Verify artifact ID is correct and artifact hasn't expired
- **`"Artifact has expired"`** - GitHub Actions artifacts expire after 90 days by default

### Notes

- Artifacts are downloaded as `.zip` files - the AI can use bash commands to extract them if needed
- Large artifact downloads have a 120-second timeout
- Artifacts must be from workflow runs in the same repository as the PR being tested
- The AI agent has full access to downloaded files in the Docker container

## Usage Patterns

### Testing PRs (Human-in-the-Loop Workflow)

```bash
# Step 1: Generate AI scenarios and export context for review
bun run start generate-tests <pr-url>             # Creates ./test-pr-{PR-number}-{commit-sha}/ directory

# Step 1 with PR-specific setup (optional)
bun run start generate-tests <pr-url> --setup     # Prompts for PR-specific setup instructions

# Step 2: Use Claude Code to refine scenarios interactively
# Run the generated interactive helper script:
./test-pr-{PR-number}-{commit-sha}/claude-refine.sh

# Step 3: Execute refined scenarios
bun run start execute-scenarios --file ./test-pr-{PR-number}-{commit-sha}/refined-scenarios.json

# Step 3 with PR-specific setup instructions (optional)
bun run start execute-scenarios --file ./test-pr-{PR-number}-{commit-sha}/refined-scenarios.json --instructions

# Step 3 with custom timeout (default: 1 minute per scenario)
bun run start execute-scenarios --file ./test-pr-{PR-number}-{commit-sha}/refined-scenarios.json --timeout 5

# Alternative: Direct execution (no human review)
# Chain commands for immediate execution:
bun run start generate-tests <pr-url> && bun run start execute-scenarios --file ./test-pr-*/generated-scenarios.json

# Custom output directory (overrides default naming)
bun run start generate-tests <pr-url> --output ./custom-dir
```

## TypeScript Configuration

- Strict mode enabled
- Node.js standard library imports
- Commander.js library for CLI framework
- Modern ES modules with top-level await

## Code Formatting

Prettier configuration:

- 2-space indentation
- 100 character line width
- Semicolons required
- Double quotes preferred
- Includes: src/
- Excludes: dist/, node_modules/

## Output Structure

Test artifacts are generated in `./test-pr-{PR-number}-{commit-sha}/` directory by default:

- **Default naming**: `./test-pr-{PR-number}-{7-char-commit-sha}/` (e.g., `./test-pr-123-abc1234/`)
- **Custom output**: Use `--output <path>` to override default naming
- **Artifacts**: Screenshots (`*.png`), Videos (`*.mp4`), QA reports
- **Context files**: `pr-analysis.json`, `generated-scenarios.json`, refinement guides

## Development Notes

- This is a Bun project with Node.js compatibility - use Node.js APIs and npm packages
- All external dependencies are managed via package.json and npm registry
- No permanent test cases - all scenarios are dynamically generated
- Unified Atlassian authentication uses single API token for both Jira and Confluence
- Always run `bun run format` and `bun run lint` before committing changes

## Code Standards

### TypeScript Configuration

- Strict mode enabled
- Node.js standard library imports
- Commander.js library for CLI framework
- Modern ES modules with top-level await

### Code Formatting (Prettier)

- 2-space indentation
- 100 character line width
- Semicolons required
- Double quotes preferred
- Includes: src/
- Excludes: dist/, node_modules/
